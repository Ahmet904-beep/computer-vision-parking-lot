{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the Notebook\n",
    "\n",
    "This notebook is focused on splitting the data for building a deep learning model for classifying parking lot images as either \"empty\" or \"not empty,\" using reinforcement learning.\n",
    "\n",
    "**Data Loading and Preprocessing**:\n",
    "- The dataset is loaded from the specified directory.\n",
    "- Images are resized, normalized, and split into training, validation, and testing sets.\n",
    "- Data is cached and prefetched for efficient pipeline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMH4mFueGiD8"
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Data manipulation and machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Enable mixed precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19ztMt8l8EFW"
   },
   "outputs": [],
   "source": [
    "# Project path\n",
    "project_path = '/content/drive/MyDrive/parkinglot_project'\n",
    "\n",
    "# Dataset paths in google drive\n",
    "dataset_path = os.path.join(project_path, 'dataset/clf-data')\n",
    "empty_path = os.path.join(dataset_path, 'empty')\n",
    "not_empty_path = os.path.join(dataset_path, 'not_empty')\n",
    "\n",
    "# Models directory\n",
    "MODELS_DIRECTORY = os.path.join(project_path, 'saved_models')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(MODELS_DIRECTORY):\n",
    "    os.makedirs(MODELS_DIRECTORY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1744326288183,
     "user": {
      "displayName": "MG tripplanner",
      "userId": "07825979804189237476"
     },
     "user_tz": -120
    },
    "id": "844fSZZ0MfUi"
   },
   "outputs": [],
   "source": [
    "# --- Set Parameters ---\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "DATASET_PATH = dataset_path\n",
    "\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1744326288225,
     "user": {
      "displayName": "MG tripplanner",
      "userId": "07825979804189237476"
     },
     "user_tz": -120
    },
    "id": "XopCXATeRBGP"
   },
   "outputs": [],
   "source": [
    "# --- Load and Preprocess Data ---\n",
    "\n",
    "def load_and_preprocess_image():\n",
    "\n",
    "    # Load full dataset and resize\n",
    "    full_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        DATASET_PATH,\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode='int',\n",
    "        shuffle=True,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    class_names = full_dataset.class_names\n",
    "    print(\"Class names:\", class_names)\n",
    "\n",
    "    for images, labels in full_dataset.take(1):\n",
    "        print(\"Image shape:\", images[0].shape)\n",
    "        print(\"Image dtype:\", images[0].dtype)\n",
    "        print(\"Pixel min/max:\", tf.reduce_min(images[0]).numpy(), tf.reduce_max(images[0]).numpy())\n",
    "\n",
    "    # Rescaling the data\n",
    "    def preprocess_image(image):\n",
    "        return tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "\n",
    "    full_dataset = full_dataset.map(lambda x, y: (preprocess_image(x), y))\n",
    "\n",
    "    print(\"After rescaling:\")\n",
    "    for images, labels in full_dataset.take(1):\n",
    "        print(\"Image shape:\", images[0].shape)\n",
    "        print(\"Image dtype:\", images[0].dtype)\n",
    "        print(\"Pixel min/max:\", tf.reduce_min(images[0]).numpy(), tf.reduce_max(images[0]).numpy())\n",
    "\n",
    "    # Get total batches\n",
    "    total_batches = tf.data.experimental.cardinality(full_dataset).numpy()\n",
    "    print ('Total batches:',total_batches )\n",
    "\n",
    "    # Split dataset into training, validation, and testing datasets\n",
    "    test_batches = math.floor(TEST_SIZE * total_batches)\n",
    "    val_batches = math.floor(VAL_SIZE * total_batches)\n",
    "\n",
    "    print (f\"Test batches: {test_batches} of {BATCH_SIZE} images per batch, for a total of {test_batches*BATCH_SIZE} images\")\n",
    "    print (f\"Val batches: {val_batches} of {BATCH_SIZE} images per batch, for a total of {val_batches*BATCH_SIZE} images\")\n",
    "    print (f\"Train batches: {total_batches - test_batches - val_batches} of {BATCH_SIZE} images per batch, for a total of {(total_batches - test_batches - val_batches)*BATCH_SIZE} images\")\n",
    "\n",
    "    assert test_batches + val_batches <= total_batches, \\\n",
    "        \"Sum of test and validation batches exceeds total number of batches\"\n",
    "\n",
    "    # Split the dataset\n",
    "    test_dataset = full_dataset.take(test_batches)\n",
    "    val_dataset = full_dataset.skip(test_batches).take(val_batches)\n",
    "    train_dataset = full_dataset.skip(test_batches + val_batches)\n",
    "\n",
    "    # Prefetch and cache data for better pipeline performance\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE).cache()\n",
    "    val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE).cache()\n",
    "    test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE).cache()\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1450,
     "status": "ok",
     "timestamp": 1744326395517,
     "user": {
      "displayName": "MG tripplanner",
      "userId": "07825979804189237476"
     },
     "user_tz": -120
    },
    "id": "57hee74S8ei5",
    "outputId": "eed6ff9d-ceeb-4df7-a850-a3b36d4768ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6090 files belonging to 2 classes.\n",
      "Class names: ['empty', 'not_empty']\n",
      "Image shape: (224, 224, 3)\n",
      "Image dtype: <dtype: 'float32'>\n",
      "Pixel min/max: 4.927328 219.40182\n",
      "After rescaling:\n",
      "Image shape: (224, 224, 3)\n",
      "Image dtype: <dtype: 'float32'>\n",
      "Pixel min/max: -0.9955686 0.7708603\n",
      "Total batches: 191\n",
      "Test batches: 28 of 32 images per batch, for a total of 896 images\n",
      "Val batches: 28 of 32 images per batch, for a total of 896 images\n",
      "Train batches: 135 of 32 images per batch, for a total of 4320 images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<CacheDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>,\n",
       " <CacheDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>,\n",
       " <CacheDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>,\n",
       " ['empty', 'not_empty'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_and_preprocess_image()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNQwYt/LUvaefM8j/SD6tJd",
   "gpuType": "L4",
   "mount_file_id": "1Vgv_Z7QKP2DlLrxdPb3BHjY3qngaDjoR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
